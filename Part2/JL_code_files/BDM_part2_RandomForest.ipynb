{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, get_scorer_names\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from scipy.stats.mstats import winsorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scikeras\n",
    "import warnings\n",
    "from tensorflow import get_logger\n",
    "get_logger().setLevel('ERROR')\n",
    "warnings.filterwarnings('ignore', message='Setting the random state for TF')\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn as sns\n",
    "\n",
    "# Set global parameters\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "plt.rcParams['figure.figsize'] = (12,5)\n",
    "plt.rcParams['figure.titlesize'] = 20\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDENAMK</th>\n",
       "      <th>GDENR</th>\n",
       "      <th>KTKZ</th>\n",
       "      <th>address</th>\n",
       "      <th>area</th>\n",
       "      <th>balcony</th>\n",
       "      <th>basement</th>\n",
       "      <th>bath</th>\n",
       "      <th>cabletv</th>\n",
       "      <th>cheminee</th>\n",
       "      <th>...</th>\n",
       "      <th>dist_to_5G</th>\n",
       "      <th>dist_to_haltst</th>\n",
       "      <th>dist_to_highway</th>\n",
       "      <th>dist_to_lake</th>\n",
       "      <th>dist_to_main_stat</th>\n",
       "      <th>dist_to_school_1</th>\n",
       "      <th>dist_to_train_stat</th>\n",
       "      <th>restaur_pix_count_km2</th>\n",
       "      <th>superm_pix_count_km2</th>\n",
       "      <th>dist_to_river</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chavornay</td>\n",
       "      <td>5749</td>\n",
       "      <td>VD</td>\n",
       "      <td>Rue de Sadaz 15, 1373 Chavornay</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12760.941970</td>\n",
       "      <td>542.609436</td>\n",
       "      <td>845.529420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>542.609436</td>\n",
       "      <td>39.051248</td>\n",
       "      <td>542.609436</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1232.655713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grenchen</td>\n",
       "      <td>2546</td>\n",
       "      <td>SO</td>\n",
       "      <td>Viaduktstrasse 8, 2540 Grenchen</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8755.851529</td>\n",
       "      <td>78.517514</td>\n",
       "      <td>2880.772987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>850.367568</td>\n",
       "      <td>413.146463</td>\n",
       "      <td>535.210239</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1124.472721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vevey</td>\n",
       "      <td>5890</td>\n",
       "      <td>VD</td>\n",
       "      <td>1800 Vevey</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6926.931211</td>\n",
       "      <td>21.931712</td>\n",
       "      <td>2209.414402</td>\n",
       "      <td>166.042100</td>\n",
       "      <td>302.702825</td>\n",
       "      <td>140.035710</td>\n",
       "      <td>302.702825</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>6661.799045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bern</td>\n",
       "      <td>351</td>\n",
       "      <td>BE</td>\n",
       "      <td>Spitalackerstrasse 16, 3013 Bern BE</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>869.835042</td>\n",
       "      <td>69.462220</td>\n",
       "      <td>1390.906539</td>\n",
       "      <td>3528.066226</td>\n",
       "      <td>1537.693077</td>\n",
       "      <td>332.987988</td>\n",
       "      <td>1537.693077</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>438.552999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Winterthur</td>\n",
       "      <td>230</td>\n",
       "      <td>ZH</td>\n",
       "      <td>Im Geissacker 40, 8404 Winterthur</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>575.800313</td>\n",
       "      <td>92.849340</td>\n",
       "      <td>2169.393694</td>\n",
       "      <td>2178.792308</td>\n",
       "      <td>2983.516382</td>\n",
       "      <td>136.619911</td>\n",
       "      <td>352.512411</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>897.649851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GDENAMK  GDENR KTKZ                              address  area  \\\n",
       "id                                                                      \n",
       "1    Chavornay   5749   VD      Rue de Sadaz 15, 1373 Chavornay  80.0   \n",
       "2     Grenchen   2546   SO      Viaduktstrasse 8, 2540 Grenchen  90.0   \n",
       "3        Vevey   5890   VD                           1800 Vevey  32.0   \n",
       "4         Bern    351   BE  Spitalackerstrasse 16, 3013 Bern BE  77.0   \n",
       "5   Winterthur    230   ZH    Im Geissacker 40, 8404 Winterthur   NaN   \n",
       "\n",
       "    balcony  basement  bath  cabletv  cheminee  ...    dist_to_5G  \\\n",
       "id                                              ...                 \n",
       "1       NaN       NaN   NaN      NaN       NaN  ...  12760.941970   \n",
       "2       1.0       NaN   NaN      1.0       NaN  ...   8755.851529   \n",
       "3       NaN       NaN   NaN      NaN       NaN  ...   6926.931211   \n",
       "4       1.0       NaN   NaN      1.0       NaN  ...    869.835042   \n",
       "5       1.0       NaN   NaN      1.0       NaN  ...    575.800313   \n",
       "\n",
       "   dist_to_haltst  dist_to_highway  dist_to_lake  dist_to_main_stat  \\\n",
       "id                                                                    \n",
       "1      542.609436       845.529420           NaN         542.609436   \n",
       "2       78.517514      2880.772987           NaN         850.367568   \n",
       "3       21.931712      2209.414402    166.042100         302.702825   \n",
       "4       69.462220      1390.906539   3528.066226        1537.693077   \n",
       "5       92.849340      2169.393694   2178.792308        2983.516382   \n",
       "\n",
       "    dist_to_school_1  dist_to_train_stat  restaur_pix_count_km2  \\\n",
       "id                                                                \n",
       "1          39.051248          542.609436                      0   \n",
       "2         413.146463          535.210239                      7   \n",
       "3         140.035710          302.702825                     45   \n",
       "4         332.987988         1537.693077                     11   \n",
       "5         136.619911          352.512411                      2   \n",
       "\n",
       "    superm_pix_count_km2  dist_to_river  \n",
       "id                                       \n",
       "1                      0    1232.655713  \n",
       "2                      0    1124.472721  \n",
       "3                      5    6661.799045  \n",
       "4                      3     438.552999  \n",
       "5                      2     897.649851  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "train = pd.read_csv('training.csv', index_col='id')\n",
    "test = pd.read_csv('X_test.csv', index_col='id')\n",
    "\n",
    "test_Y = pd.read_csv('Y_test_example.csv', index_col='id')\n",
    "# for the submission, overwrite all y value with na\n",
    "# will populate with our prediction later\n",
    "test_Y['rent_full'] = np.nan\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.drop_duplicates() \n",
    "# no duplicates found. good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDENR</th>\n",
       "      <th>area</th>\n",
       "      <th>balcony</th>\n",
       "      <th>basement</th>\n",
       "      <th>bath</th>\n",
       "      <th>cabletv</th>\n",
       "      <th>cheminee</th>\n",
       "      <th>dishwasher</th>\n",
       "      <th>dryer</th>\n",
       "      <th>elevator</th>\n",
       "      <th>...</th>\n",
       "      <th>dist_to_5G</th>\n",
       "      <th>dist_to_haltst</th>\n",
       "      <th>dist_to_highway</th>\n",
       "      <th>dist_to_lake</th>\n",
       "      <th>dist_to_main_stat</th>\n",
       "      <th>dist_to_school_1</th>\n",
       "      <th>dist_to_train_stat</th>\n",
       "      <th>restaur_pix_count_km2</th>\n",
       "      <th>superm_pix_count_km2</th>\n",
       "      <th>dist_to_river</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>72000.000000</td>\n",
       "      <td>57767.000000</td>\n",
       "      <td>34520.0</td>\n",
       "      <td>976.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>24906.0</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>21939.0</td>\n",
       "      <td>...</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>71492.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>38262.000000</td>\n",
       "      <td>50571.000000</td>\n",
       "      <td>71498.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2879.512722</td>\n",
       "      <td>82.959233</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7638.035391</td>\n",
       "      <td>166.852546</td>\n",
       "      <td>2922.607580</td>\n",
       "      <td>1864.570974</td>\n",
       "      <td>1236.536863</td>\n",
       "      <td>396.477677</td>\n",
       "      <td>1057.297004</td>\n",
       "      <td>11.650417</td>\n",
       "      <td>1.721319</td>\n",
       "      <td>823.578550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2199.443778</td>\n",
       "      <td>34.487208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12363.592303</td>\n",
       "      <td>174.026725</td>\n",
       "      <td>3228.381560</td>\n",
       "      <td>5873.068385</td>\n",
       "      <td>1155.591313</td>\n",
       "      <td>355.310688</td>\n",
       "      <td>1158.211097</td>\n",
       "      <td>22.482845</td>\n",
       "      <td>2.219454</td>\n",
       "      <td>968.968316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>11.401754</td>\n",
       "      <td>4.638762</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>404.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1323.998583</td>\n",
       "      <td>78.873316</td>\n",
       "      <td>1145.318296</td>\n",
       "      <td>576.187827</td>\n",
       "      <td>474.067506</td>\n",
       "      <td>204.732142</td>\n",
       "      <td>374.001003</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>224.010704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2766.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3529.439957</td>\n",
       "      <td>130.387883</td>\n",
       "      <td>1984.648834</td>\n",
       "      <td>1204.365110</td>\n",
       "      <td>885.849874</td>\n",
       "      <td>327.678196</td>\n",
       "      <td>684.942698</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>515.221416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4776.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9033.896515</td>\n",
       "      <td>202.141040</td>\n",
       "      <td>3410.488088</td>\n",
       "      <td>2176.086622</td>\n",
       "      <td>1613.165521</td>\n",
       "      <td>473.608488</td>\n",
       "      <td>1243.497085</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1065.183279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6810.000000</td>\n",
       "      <td>945.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>88563.904680</td>\n",
       "      <td>4967.810886</td>\n",
       "      <td>41454.525950</td>\n",
       "      <td>205028.994100</td>\n",
       "      <td>15405.006360</td>\n",
       "      <td>7534.579285</td>\n",
       "      <td>23787.109950</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8233.113960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              GDENR          area  balcony  basement   bath  cabletv  \\\n",
       "count  72000.000000  57767.000000  34520.0     976.0  496.0  24906.0   \n",
       "mean    2879.512722     82.959233      1.0       1.0    1.0      1.0   \n",
       "std     2199.443778     34.487208      0.0       0.0    0.0      0.0   \n",
       "min        1.000000      1.000000      1.0       1.0    1.0      1.0   \n",
       "25%      404.000000     60.000000      1.0       1.0    1.0      1.0   \n",
       "50%     2766.000000     80.000000      1.0       1.0    1.0      1.0   \n",
       "75%     4776.000000    101.000000      1.0       1.0    1.0      1.0   \n",
       "max     6810.000000    945.000000      1.0       1.0    1.0      1.0   \n",
       "\n",
       "       cheminee  dishwasher  dryer  elevator  ...    dist_to_5G  \\\n",
       "count    3330.0      1178.0  572.0   21939.0  ...  72000.000000   \n",
       "mean        1.0         1.0    1.0       1.0  ...   7638.035391   \n",
       "std         0.0         0.0    0.0       0.0  ...  12363.592303   \n",
       "min         1.0         1.0    1.0       1.0  ...      2.236068   \n",
       "25%         1.0         1.0    1.0       1.0  ...   1323.998583   \n",
       "50%         1.0         1.0    1.0       1.0  ...   3529.439957   \n",
       "75%         1.0         1.0    1.0       1.0  ...   9033.896515   \n",
       "max         1.0         1.0    1.0       1.0  ...  88563.904680   \n",
       "\n",
       "       dist_to_haltst  dist_to_highway   dist_to_lake  dist_to_main_stat  \\\n",
       "count    71492.000000     72000.000000   38262.000000       50571.000000   \n",
       "mean       166.852546      2922.607580    1864.570974        1236.536863   \n",
       "std        174.026725      3228.381560    5873.068385        1155.591313   \n",
       "min          2.828427        11.401754       4.638762           3.605551   \n",
       "25%         78.873316      1145.318296     576.187827         474.067506   \n",
       "50%        130.387883      1984.648834    1204.365110         885.849874   \n",
       "75%        202.141040      3410.488088    2176.086622        1613.165521   \n",
       "max       4967.810886     41454.525950  205028.994100       15405.006360   \n",
       "\n",
       "       dist_to_school_1  dist_to_train_stat  restaur_pix_count_km2  \\\n",
       "count      71498.000000        72000.000000           72000.000000   \n",
       "mean         396.477677         1057.297004              11.650417   \n",
       "std          355.310688         1158.211097              22.482845   \n",
       "min            1.414214            3.162278               0.000000   \n",
       "25%          204.732142          374.001003               1.000000   \n",
       "50%          327.678196          684.942698               3.000000   \n",
       "75%          473.608488         1243.497085               9.000000   \n",
       "max         7534.579285        23787.109950             136.000000   \n",
       "\n",
       "       superm_pix_count_km2  dist_to_river  \n",
       "count          72000.000000   72000.000000  \n",
       "mean               1.721319     823.578550  \n",
       "std                2.219454     968.968316  \n",
       "min                0.000000       0.041264  \n",
       "25%                0.000000     224.010704  \n",
       "50%                1.000000     515.221416  \n",
       "75%                3.000000    1065.183279  \n",
       "max               12.000000    8233.113960  \n",
       "\n",
       "[8 rows x 76 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary statistics\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72000 entries, 1 to 72000\n",
      "Data columns (total 82 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   GDENAMK                         72000 non-null  object \n",
      " 1   GDENR                           72000 non-null  int64  \n",
      " 2   KTKZ                            72000 non-null  object \n",
      " 3   address                         72000 non-null  object \n",
      " 4   area                            57767 non-null  float64\n",
      " 5   balcony                         34520 non-null  float64\n",
      " 6   basement                        976 non-null    float64\n",
      " 7   bath                            496 non-null    float64\n",
      " 8   cabletv                         24906 non-null  float64\n",
      " 9   cheminee                        3330 non-null   float64\n",
      " 10  date                            72000 non-null  object \n",
      " 11  descr                           51473 non-null  object \n",
      " 12  dishwasher                      1178 non-null   float64\n",
      " 13  dryer                           572 non-null    float64\n",
      " 14  elevator                        21939 non-null  float64\n",
      " 15  floors                          43689 non-null  float64\n",
      " 16  furnished                       195 non-null    float64\n",
      " 17  gardenshed                      13 non-null     float64\n",
      " 18  heating_air                     36 non-null     float64\n",
      " 19  heating_earth                   18 non-null     float64\n",
      " 20  heating_electro                 12 non-null     float64\n",
      " 21  heating_far                     41 non-null     float64\n",
      " 22  heating_gas                     49 non-null     float64\n",
      " 23  heating_oil                     71 non-null     float64\n",
      " 24  heating_pellets                 11 non-null     float64\n",
      " 25  home_type                       72000 non-null  object \n",
      " 26  kids_friendly                   13655 non-null  float64\n",
      " 27  lat                             72000 non-null  float64\n",
      " 28  laundry                         606 non-null    float64\n",
      " 29  lon                             72000 non-null  float64\n",
      " 30  manlift                         1 non-null      float64\n",
      " 31  middle_house                    12 non-null     float64\n",
      " 32  month                           72000 non-null  int64  \n",
      " 33  msregion                        72000 non-null  int64  \n",
      " 34  newly_built                     72000 non-null  int64  \n",
      " 35  oldbuilding                     2563 non-null   float64\n",
      " 36  oven                            9 non-null      float64\n",
      " 37  parking_indoor                  18809 non-null  float64\n",
      " 38  parking_outside                 18871 non-null  float64\n",
      " 39  playground                      221 non-null    float64\n",
      " 40  pool                            26 non-null     float64\n",
      " 41  quarter_general                 72000 non-null  int64  \n",
      " 42  quarter_specific                72000 non-null  float64\n",
      " 43  quiet                           45 non-null     float64\n",
      " 44  raised_groundfloor              715 non-null    float64\n",
      " 45  rent_full                       72000 non-null  int64  \n",
      " 46  rooms                           70155 non-null  float64\n",
      " 47  shower                          11 non-null     float64\n",
      " 48  sunny                           54 non-null     float64\n",
      " 49  terrace                         102 non-null    float64\n",
      " 50  toilets                         982 non-null    float64\n",
      " 51  topstorage                      124 non-null    float64\n",
      " 52  veranda                         74 non-null     float64\n",
      " 53  water                           91 non-null     float64\n",
      " 54  year                            72000 non-null  int64  \n",
      " 55  year_built                      23749 non-null  float64\n",
      " 56  Micro_rating                    72000 non-null  float64\n",
      " 57  Micro_rating_NoiseAndEmission   72000 non-null  float64\n",
      " 58  Micro_rating_Accessibility      72000 non-null  float64\n",
      " 59  Micro_rating_DistrictAndArea    72000 non-null  float64\n",
      " 60  Micro_rating_SunAndView         72000 non-null  int64  \n",
      " 61  Micro_rating_ServicesAndNature  72000 non-null  float64\n",
      " 62  wgh_avg_sonnenklasse_per_egid   71985 non-null  float64\n",
      " 63  Anteil_auslaend                 69854 non-null  float64\n",
      " 64  Avg_age                         69854 non-null  float64\n",
      " 65  Avg_size_household              69767 non-null  float64\n",
      " 66  Noise_max                       72000 non-null  int64  \n",
      " 67  anteil_efh                      70439 non-null  float64\n",
      " 68  apoth_pix_count_km2             72000 non-null  int64  \n",
      " 69  avg_anzhl_geschosse             70437 non-null  float64\n",
      " 70  avg_bauperiode                  70437 non-null  float64\n",
      " 71  dist_to_4G                      72000 non-null  float64\n",
      " 72  dist_to_5G                      72000 non-null  float64\n",
      " 73  dist_to_haltst                  71492 non-null  float64\n",
      " 74  dist_to_highway                 72000 non-null  float64\n",
      " 75  dist_to_lake                    38262 non-null  float64\n",
      " 76  dist_to_main_stat               50571 non-null  float64\n",
      " 77  dist_to_school_1                71498 non-null  float64\n",
      " 78  dist_to_train_stat              72000 non-null  float64\n",
      " 79  restaur_pix_count_km2           72000 non-null  int64  \n",
      " 80  superm_pix_count_km2            72000 non-null  int64  \n",
      " 81  dist_to_river                   72000 non-null  float64\n",
      "dtypes: float64(64), int64(12), object(6)\n",
      "memory usage: 45.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# how many non-missing values per feature\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist(np.log2(train['year_built']), bins=30, density=True)\n",
    "plt.hist(train['year_built'], bins=30, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='rooms', ylabel='area'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.plot.scatter(x='rooms', y='area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_clean_data(data):\n",
    "    df = data.drop(['GDENAMK', 'year', 'date', 'lat', 'lon', ], axis=1)\n",
    "    # GDENAMK, we have id: GDENR, we use msregion as best location indicator.\n",
    "    # quarter_specific not needed, we have year and quarter (and date) already\n",
    "    # 'year' listed is always 2019, redundant\n",
    "\n",
    "    # drop least useful location indicator\n",
    "    df = df.drop(['GDENR', 'KTKZ', 'address'], axis=1)\n",
    "\n",
    "    # drop dummy columns with many missing values\n",
    "    # dropped due to low data quality => low predictive power\n",
    "    # floors has negative values and quite a few missing.\n",
    "    df = df.drop(['basement', 'bath', 'cheminee', 'dishwasher', 'dryer', \n",
    "                'furnished', 'gardenshed', 'heating_air', 'heating_earth', \n",
    "                'heating_electro', 'heating_far', 'heating_gas', 'heating_oil', \n",
    "                'heating_pellets', 'kids_friendly', 'laundry', 'manlift', 'middle_house', \n",
    "                'oven', 'playground', 'pool', 'quiet', 'raised_groundfloor', 'shower',\n",
    "                'sunny', 'terrace', 'toilets', 'topstorage', 'veranda', 'water', 'oldbuilding', ], axis=1)\n",
    "\n",
    "    # the following have over 10'000 positive answers. We keep for the prediction.\n",
    "    for c in ['balcony', 'cabletv', 'elevator', 'parking_indoor', 'parking_outside', ]:\n",
    "        df[c] = np.where(np.isnan(df[c]), 0, 1)\n",
    "\n",
    "    # impute area based on msregion, home_type, and number of rooms => median b.c. outliers in area\n",
    "    df['area'] = df['area'].fillna(df.groupby(['home_type', 'rooms',])['area'].transform('median'))\n",
    "\n",
    "    # # replace description text with dummy, if description exists or not\n",
    "    df = df.astype({'descr': str})\n",
    "    df['descr'] = np.where(df.descr=='nan', 0, df.descr.str.len())\n",
    "\n",
    "    # has any parking\n",
    "    df['parking_any'] = np.where((df.parking_indoor==1) & (df.parking_outside==1), 0, 1)\n",
    "\n",
    "    # impute floors with average\n",
    "    df['floors'] = df['floors'].fillna(df['avg_anzhl_geschosse'])\n",
    "\n",
    "    # impute year_built\n",
    "    df['year_built'] = np.where(df['year_built']==-1 , np.nan, df['year_built'])\n",
    "    df['year_built'] = df['year_built'].fillna(df['avg_bauperiode'])\n",
    "    df['year_built'] = df['year_built']**20 # fix left skew\n",
    "\n",
    "    # log transform right skewed distributions\n",
    "    for f in ['Noise_max', 'dist_to_4G', 'dist_to_5G']:\n",
    "        df[f] = df[f]+1\n",
    "\n",
    "    for f in ['wgh_avg_sonnenklasse_per_egid', 'Avg_age', 'Avg_size_household',\n",
    "            'Noise_max', 'avg_anzhl_geschosse', 'dist_to_4G', \n",
    "            'dist_to_5G','dist_to_haltst', 'dist_to_river' ]:\n",
    "        df[f] = np.log(df[f])\n",
    "\n",
    "    # standardize non-dummy features\n",
    "    # winsorize some variables with outliers\n",
    "    for f in ['Micro_rating', 'Micro_rating_NoiseAndEmission', 'Micro_rating_Accessibility',\n",
    "              'Micro_rating_DistrictAndArea', 'Micro_rating_SunAndView', 'Micro_rating_ServicesAndNature',\n",
    "              'wgh_avg_sonnenklasse_per_egid', 'Anteil_auslaend', 'Avg_age', 'Avg_size_household',\n",
    "              'Avg_size_household', 'Noise_max', 'avg_anzhl_geschosse', 'avg_bauperiode',\n",
    "              'dist_to_4G', 'dist_to_5G', 'dist_to_haltst', 'dist_to_highway', 'dist_to_school_1',\n",
    "              'dist_to_train_stat', 'dist_to_river', 'year_built']:\n",
    "        df[f] = winsorize(df[f], limits=[0.03, 0.03], nan_policy='propagate')   \n",
    "\n",
    "    # the following variables are categorical or the numerical representation is not meaningful \n",
    "    # (e.g. larger numbers not systematically different from smaller numbers)\n",
    "    # transform into dummies\n",
    "    for f in ['msregion', 'home_type', 'rooms']:\n",
    "        dummies = pd.get_dummies(df[f], prefix=str(f), drop_first=False) \n",
    "        df = df.merge(dummies, left_index=True, right_index=True)\n",
    "        df = df.drop(f, axis=1) # drop the original variable\n",
    "\n",
    "    # other vars we don't want, mostly because too little data or no meaningful improvement by including\n",
    "    df = df.drop(['dist_to_lake', 'dist_to_main_stat', 'quarter_specific', \n",
    "                'quarter_general', 'month', 'anteil_efh', 'avg_anzhl_geschosse', \n",
    "                'avg_bauperiode', ], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "df_train = prepare_clean_data(train)\n",
    "df_test = prepare_clean_data(test)\n",
    "\n",
    "# ensure test data has all the right columns\n",
    "# Get missing columns in the training test\n",
    "missing_cols = set( df_train.columns ) - set( df_test.columns )\n",
    "\n",
    "# Add a missing column in test set with default value equal to 0\n",
    "for c in missing_cols:\n",
    "    df_test[c] = 0 # constant\n",
    "\n",
    "# Ensure the order of column in the test set is in the same order than in train set\n",
    "df_test = df_test[df_train.columns]\n",
    "df_test = df_test.drop('rent_full', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize train and test data (except the dummies)\n",
    "cols_to_scale = ['area',\n",
    "                 'Micro_rating', 'Micro_rating_NoiseAndEmission', 'Micro_rating_Accessibility',\n",
    "                 'wgh_avg_sonnenklasse_per_egid', 'Anteil_auslaend', 'Avg_age', 'Avg_size_household',\n",
    "                 'Avg_size_household', 'Noise_max', 'descr', 'dist_to_4G', 'dist_to_5G',\n",
    "                 'dist_to_haltst', 'dist_to_highway', 'dist_to_school_1',\n",
    "                 'dist_to_train_stat', 'dist_to_river', 'year_built' ]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cols_to_scale])\n",
    "df_train[cols_to_scale] = scaler.transform(df_train[cols_to_scale])\n",
    "\n",
    "# scale test data with the fit from train data\n",
    "df_test[cols_to_scale] = scaler.transform(df_test[cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle rows with missing data\n",
    "    \n",
    "# training    \n",
    "df_train = df_train.dropna()\n",
    "\n",
    "# testing\n",
    "temp = df_test.copy()\n",
    "df_test = temp.dropna() # we only predict on the rows that are possible\n",
    "\n",
    "# we safe the index of the test data for which we have missing data\n",
    "df_test_missing = temp.merge(df_test, left_index=True, right_index=True, how='left', indicator=True)\n",
    "df_test_missing = df_test_missing[df_test_missing['_merge'] == 'left_only']\n",
    "df_test_missing = df_test_missing[[]] # only keep the index column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train values for X and y\n",
    "X = df_train.drop(['rent_full'], axis=1).values\n",
    "y = df_train.rent_full.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (55255, 168)\n",
      "Shape of y_train: (55255,)\n",
      "Shape of X_valid: (13814, 168)\n",
      "Shape of y_valid: (13814,)\n"
     ]
    }
   ],
   "source": [
    "# set aside validation data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=1)\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_valid:\", X_valid.shape)\n",
    "print(\"Shape of y_valid:\", y_valid.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 168, Obs: 69'069, MSE: 139'574\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LinearRegression(), X_train, y_train, scoring='neg_mean_squared_error', cv=KFold(n_splits=10))\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Features: {len(df_train.columns)-1}, Obs: {len(df_train):_}, MSE: {int(-1*avg_score):_}'.replace('_', \"'\"))\n",
    "\n",
    "# all 'bad' dummies included and NA coded as 0, Msregion only\n",
    "# =>                                                    Features: 214, Obs: 55'209, MSE: 361'972'499'893\n",
    "\n",
    "# No 'bad' dummies                                      Features: 177, Obs: 55'209, MSE: 142'565\n",
    "# some 'bad' dummies (>10'000 responses)                Features: 182, Obs: 55'209, MSE: 139'787\n",
    "\n",
    "# msregion and KTKZ:                                    Features: 208, Obs: 55'209, MSE: 139'779\n",
    "# ==> msregion only b.c. fewer features\n",
    "\n",
    "# area imputed (type, rooms):                           Features: 182, Obs: 68'136, MSE: 141'183\n",
    "# area imputed (msregion, type, rooms):                 Features: 182, Obs: 67'890, MSE: 140'556\n",
    "# ==> more observations. keep\n",
    "# ==> TODO, which one better? kept the one without msregion\n",
    "\n",
    "# added dummy for description                           Features: 183, Obs: 68'136, MSE: 141'173\n",
    "# added description length                              Features: 183, Obs: 68'136, MSE: 140'855\n",
    "# description length helpful \n",
    "\n",
    "# dummy for any parking                                 Features: 184, Obs: 68'136, MSE: 140'849\n",
    "# slightly better\n",
    "\n",
    "# quarter and month of listing ignored.                 Features: 169, Obs: 68'136, MSE: 140'886\n",
    "# ==> same mse with fewer features\n",
    "\n",
    "# drop anteil_efh                                       Features: 168, Obs: 68'136, MSE: 140'881\n",
    "\n",
    "# standardize vars:                                     Features: 168, Obs: 68'136, MSE: 140'881\n",
    "# ==> no impact, as expected\n",
    "\n",
    "# log transform for better normal distribution          Features: 168, Obs: 68'136, MSE: 140'727\n",
    "# => slightly better\n",
    "# log of area makes mse much worse. too much discounted\n",
    "\n",
    "# winsorize all (expect area)\n",
    "# (3%, 3%):                                             Features: 168, Obs: 69'069, MSE: 140'595\n",
    "# (5%, 5%):                                             Features: 168, Obs: 71'276, MSE: 143'470\n",
    "# (1%, 1%):                                             Features: 168, Obs: 69'069, MSE: 140'637\n",
    "# => 3% symmetric winsorization\n",
    "\n",
    "# impute floors from average_geschosse and drop that average\n",
    "#                                                        Features: 168, Obs: 69'069, MSE: 140'549       \n",
    "# ==> floors could be better than average, as more precise, use that \n",
    "\n",
    "# impute year_built from average and drop said average\n",
    "#                                                        Features: 168, Obs: 69'069, MSE: 140'504\n",
    "# with left skew corrected and standardized              Features: 168, Obs: 69'069, MSE: 139'987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 168, Obs: 69'069, MSE: 142'652\n"
     ]
    }
   ],
   "source": [
    "# fit OLS model\n",
    "ols = LinearRegression().fit(X_train, y_train)\n",
    "y_hat = ols.predict(X_valid)\n",
    "print(f'Features: {len(df_train.columns)-1}, Obs: {len(df_train):_}, MSE: {int(mean_squared_error(y_valid, y_hat)):_}'.replace('_', \"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "showcase = pd.DataFrame(y_hat, y_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random grid\n",
    "parameters = {'n_estimators': [500, 750, 1000],\n",
    "               'max_depth': [100, 150, None],\n",
    "               'min_samples_split': [1, 5, 10],\n",
    "               'min_samples_leaf': [1, 5, 10],\n",
    "               'criterion': ['squared_error'],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random search use a very small subset of train data\n",
    "X_trn, _, y_trn, _ = train_test_split(X_train, y_train, train_size=0.05, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/sklearn/model_selection/_search.py:306: UserWarning: The total space of parameters 81 is smaller than n_iter=100. Running 81 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=1, n_estimators=500; total time=   1.5s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=1, n_estimators=500; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=1, n_estimators=500; total time=   1.5s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=1, n_estimators=750; total time=   1.7s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=1, n_estimators=750; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   4.0s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   2.8s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   3.0s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time=   4.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time=   4.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time=   4.5s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   5.8s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   5.7s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   5.9s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.7s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.7s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.8s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=750; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=750; total time=   4.0s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=750; total time=   4.2s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   5.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   5.1s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   5.1s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=1, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=1, n_estimators=500; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=1, n_estimators=500; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=1, n_estimators=750; total time=   1.6s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=1, n_estimators=750; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=5, n_estimators=500; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=5, n_estimators=500; total time=   2.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=5, n_estimators=500; total time=   2.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=5, n_estimators=750; total time=   3.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=5, n_estimators=750; total time=   3.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=5, n_estimators=750; total time=   3.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time=   4.1s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time=   4.1s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time=   4.6s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=10, n_estimators=500; total time=   2.2s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=10, n_estimators=500; total time=   2.2s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=10, n_estimators=500; total time=   2.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=10, n_estimators=750; total time=   3.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=10, n_estimators=750; total time=   3.5s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=10, n_estimators=750; total time=  12.7s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=10, n_estimators=1000; total time=   4.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=10, n_estimators=1000; total time=   4.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=5, min_samples_split=10, n_estimators=1000; total time=   4.6s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=1, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=1, n_estimators=500; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=1, n_estimators=500; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=1, n_estimators=1000; total time=   1.5s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=1, n_estimators=1000; total time=   1.5s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   3.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=5, n_estimators=750; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=5, n_estimators=750; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=5, n_estimators=750; total time=   2.8s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=5, n_estimators=1000; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=5, n_estimators=1000; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=5, n_estimators=1000; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=10, n_estimators=500; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=10, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=10, n_estimators=500; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=10, n_estimators=750; total time=   2.8s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=10, n_estimators=750; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=10, n_estimators=750; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=10, n_estimators=1000; total time=   3.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=10, n_estimators=1000; total time=   3.3s\n",
      "[CV] END criterion=squared_error, max_depth=100, min_samples_leaf=10, min_samples_split=10, n_estimators=1000; total time=   3.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=1, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=1, n_estimators=500; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=1, n_estimators=500; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   4.1s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time=   4.6s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time=   4.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time=   4.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   5.8s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   5.8s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   6.3s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.7s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.6s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.8s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=10, n_estimators=750; total time=   4.0s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=10, n_estimators=750; total time=   4.0s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=10, n_estimators=750; total time=   4.1s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   5.0s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   5.5s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   5.6s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=1, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=1, n_estimators=500; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=1, n_estimators=500; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=1, n_estimators=750; total time=   1.7s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=1, n_estimators=750; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=1, n_estimators=750; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=5, n_estimators=500; total time=   3.6s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=5, n_estimators=500; total time=   2.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=5, n_estimators=500; total time=   2.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=5, n_estimators=750; total time=   3.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=5, n_estimators=750; total time=   3.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=5, n_estimators=750; total time=   3.3s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time=   4.0s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time=   4.1s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time=   4.1s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=10, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=10, n_estimators=500; total time=   2.1s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=10, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=10, n_estimators=750; total time=   3.1s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=10, n_estimators=750; total time=   3.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=10, n_estimators=750; total time=   3.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=10, n_estimators=1000; total time=   4.0s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=10, n_estimators=1000; total time=   4.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=5, min_samples_split=10, n_estimators=1000; total time=   4.3s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=1, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=1, n_estimators=500; total time=   1.0s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=1, n_estimators=500; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=1, n_estimators=750; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=1, n_estimators=750; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   3.1s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=5, n_estimators=750; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=5, n_estimators=750; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=5, n_estimators=750; total time=   2.8s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=5, n_estimators=1000; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=5, n_estimators=1000; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=5, n_estimators=1000; total time=   3.9s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=10, n_estimators=500; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=10, n_estimators=500; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=10, n_estimators=500; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=10, n_estimators=750; total time=   2.8s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=10, n_estimators=750; total time=   2.8s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=10, n_estimators=750; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=10, n_estimators=1000; total time=   3.5s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=10, n_estimators=1000; total time=   4.0s\n",
      "[CV] END criterion=squared_error, max_depth=150, min_samples_leaf=10, min_samples_split=10, n_estimators=1000; total time=   4.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=1, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=1, n_estimators=500; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=1, n_estimators=500; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   4.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   2.8s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time=   4.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time=   4.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time=   4.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   5.7s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   5.8s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   6.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.7s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.7s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.7s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=750; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=750; total time=   3.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=750; total time=   4.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   5.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   5.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   5.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=1, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=1, n_estimators=500; total time=   1.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=1, n_estimators=500; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=1, n_estimators=750; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=1, n_estimators=750; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=1, n_estimators=750; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=1, n_estimators=1000; total time=   1.6s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=500; total time=   3.6s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=500; total time=   2.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=500; total time=   2.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=750; total time=   3.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=750; total time=   3.6s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=750; total time=   3.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time=   4.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time=   4.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time=   4.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=500; total time=   2.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=500; total time=   2.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=750; total time=   4.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=750; total time=   3.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=750; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=1000; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=1000; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=1000; total time=   4.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=1, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=1, n_estimators=500; total time=   1.2s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=1, n_estimators=500; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=1, n_estimators=750; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=1, n_estimators=750; total time=   1.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=1, n_estimators=750; total time=   1.5s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=1, n_estimators=1000; total time=   1.4s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=1, n_estimators=1000; total time=   1.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   3.3s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   2.1s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=750; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=750; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=750; total time=   2.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=1000; total time=   3.6s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=1000; total time=   3.8s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=1000; total time=   3.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=500; total time=   1.9s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=750; total time=   2.8s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=750; total time=   2.8s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=750; total time=   2.7s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=1000; total time=   3.6s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=1000; total time=   3.7s\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=1000; total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "81 fits failed out of a total of 243.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "81 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/joblib/parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/joblib/parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 189, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 1342, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 265, in fit\n",
      "    check_scalar(\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split == 1, must be >= 2.\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 476, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/joblib/parallel.py\", line 1061, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/joblib/parallel.py\", line 938, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "ValueError: min_samples_split == 1, must be >= 2.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.60607666 0.60789206 0.60720714\n",
      " 0.60666071 0.60576649 0.60641038        nan        nan        nan\n",
      " 0.58903419 0.59017504 0.590265   0.5906134  0.58975691 0.59026961\n",
      "        nan        nan        nan 0.56921028 0.56938564 0.56997257\n",
      " 0.56908144 0.56872383 0.57004681        nan        nan        nan\n",
      " 0.60726568 0.60741332 0.60654741 0.60436817 0.60500961 0.60373631\n",
      "        nan        nan        nan 0.58937283 0.58996861 0.59063655\n",
      " 0.5883157  0.58933528 0.59085373        nan        nan        nan\n",
      " 0.56949749 0.57029848 0.56988453 0.56942038 0.56997943 0.57107602\n",
      "        nan        nan        nan 0.60669065 0.60842925 0.60791546\n",
      " 0.6066504  0.60614553 0.60565359        nan        nan        nan\n",
      " 0.58980174 0.58859976 0.59171063 0.59110146 0.58987958 0.59037389\n",
      "        nan        nan        nan 0.56931133 0.56840041 0.57019611\n",
      " 0.56895497 0.57006301 0.56926595]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=RandomForestRegressor(n_jobs=-1), n_iter=100,\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;squared_error&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: [100, 150, None],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 5, 10],\n",
       "                                        &#x27;min_samples_split&#x27;: [1, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [500, 750, 1000]},\n",
       "                   random_state=42, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=RandomForestRegressor(n_jobs=-1), n_iter=100,\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;squared_error&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: [100, 150, None],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 5, 10],\n",
       "                                        &#x27;min_samples_split&#x27;: [1, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [500, 750, 1000]},\n",
       "                   random_state=42, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_jobs=-1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_jobs=-1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestRegressor(n_jobs=-1), n_iter=100,\n",
       "                   param_distributions={'criterion': ['squared_error'],\n",
       "                                        'max_depth': [100, 150, None],\n",
       "                                        'min_samples_leaf': [1, 5, 10],\n",
       "                                        'min_samples_split': [1, 5, 10],\n",
       "                                        'n_estimators': [500, 750, 1000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_jobs = -1)\n",
    "# pick 81 potential subjects\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = parameters, n_iter = 81, cv = 3, verbose=2, random_state=42)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 750,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_depth': None,\n",
       " 'criterion': 'squared_error'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick in the region of the best random parameters to fine tune.\n",
    "# Create the random grid\n",
    "parameters = {'n_estimators': [700, 750, 800],\n",
    "               'max_depth': [200, None],\n",
    "               'min_samples_split': [4, 5, 6],\n",
    "               'min_samples_leaf': [1, 2],\n",
    "               'criterion': ['squared_error'],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the scoring function in the GridSearchCV\n",
    "grid = GridSearchCV(RandomForestRegressor(n_jobs=-1), parameters, refit=False, cv=3, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=4, n_estimators=700; total time= 1.1min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=4, n_estimators=700; total time= 1.0min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=4, n_estimators=700; total time= 1.0min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=4, n_estimators=750; total time= 1.1min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=4, n_estimators=750; total time= 1.1min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=4, n_estimators=750; total time= 1.1min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=4, n_estimators=800; total time= 1.2min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=4, n_estimators=800; total time= 1.2min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=4, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=5, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=5, n_estimators=700; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=5, n_estimators=700; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time= 1.8min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time= 1.8min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=6, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=6, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=6, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=6, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=6, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=6, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=6, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=6, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=1, min_samples_split=6, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=4, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=4, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=4, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=4, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=4, n_estimators=750; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=4, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=4, n_estimators=800; total time= 1.8min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=4, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=4, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=5, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=5, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=5, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=5, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=5, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=5, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=6, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=6, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=6, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=6, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=6, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=6, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=6, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=6, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=200, min_samples_leaf=2, min_samples_split=6, n_estimators=800; total time= 1.8min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=700; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=4, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=6, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=6, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=6, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=6, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=6, n_estimators=750; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=6, n_estimators=750; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=6, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=6, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=1, min_samples_split=6, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=4, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=4, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=4, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=4, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=4, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=4, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=4, n_estimators=800; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=4, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=4, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=700; total time= 1.3min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=700; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=750; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=750; total time= 1.4min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=6, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=6, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=6, n_estimators=700; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=6, n_estimators=750; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=6, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=6, n_estimators=750; total time= 1.5min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=6, n_estimators=800; total time= 1.6min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=6, n_estimators=800; total time= 1.7min\n",
      "[CV] END criterion=squared_error, max_depth=None, min_samples_leaf=2, min_samples_split=6, n_estimators=800; total time= 1.7min\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid.fit(X_train, y_train) # here we fit with all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'squared_error',\n",
       " 'max_depth': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 700}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators= grid_result.best_params_[\"n_estimators\"],\n",
    "                              max_depth= grid_result.best_params_[\"max_depth\"],\n",
    "                              min_samples_split= grid_result.best_params_[\"min_samples_split\"],\n",
    "                              min_samples_leaf= grid_result.best_params_[\"min_samples_leaf\"] ,\n",
    "                              verbose=2,\n",
    "                              n_jobs=-1,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 700building tree 2 of 700\n",
      "\n",
      "building tree 3 of 700\n",
      "building tree 4 of 700\n",
      "building tree 5 of 700\n",
      "building tree 6 of 700\n",
      "building tree 7 of 700\n",
      "building tree 8 of 700\n",
      "building tree 9 of 700\n",
      "building tree 10 of 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 11 of 700\n",
      "building tree 12 of 700\n",
      "building tree 13 of 700\n",
      "building tree 14 of 700\n",
      "building tree 15 of 700\n",
      "building tree 16 of 700\n",
      "building tree 17 of 700\n",
      "building tree 18 of 700\n",
      "building tree 19 of 700\n",
      "building tree 20 of 700\n",
      "building tree 21 of 700\n",
      "building tree 22 of 700\n",
      "building tree 23 of 700\n",
      "building tree 24 of 700\n",
      "building tree 25 of 700\n",
      "building tree 26 of 700\n",
      "building tree 27 of 700\n",
      "building tree 28 of 700\n",
      "building tree 29 of 700\n",
      "building tree 30 of 700\n",
      "building tree 31 of 700\n",
      "building tree 32 of 700\n",
      "building tree 33 of 700\n",
      "building tree 34 of 700\n",
      "building tree 35 of 700\n",
      "building tree 36 of 700\n",
      "building tree 37 of 700\n",
      "building tree 38 of 700\n",
      "building tree 39 of 700\n",
      "building tree 40 of 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    6.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 41 of 700\n",
      "building tree 42 of 700\n",
      "building tree 43 of 700\n",
      "building tree 44 of 700\n",
      "building tree 45 of 700\n",
      "building tree 46 of 700\n",
      "building tree 47 of 700\n",
      "building tree 48 of 700\n",
      "building tree 49 of 700\n",
      "building tree 50 of 700\n",
      "building tree 51 of 700\n",
      "building tree 52 of 700\n",
      "building tree 53 of 700\n",
      "building tree 54 of 700\n",
      "building tree 55 of 700\n",
      "building tree 56 of 700\n",
      "building tree 57 of 700\n",
      "building tree 58 of 700\n",
      "building tree 59 of 700\n",
      "building tree 60 of 700\n",
      "building tree 61 of 700\n",
      "building tree 62 of 700\n",
      "building tree 63 of 700\n",
      "building tree 64 of 700\n",
      "building tree 65 of 700\n",
      "building tree 66 of 700\n",
      "building tree 67 of 700\n",
      "building tree 68 of 700\n",
      "building tree 69 of 700\n",
      "building tree 70 of 700\n",
      "building tree 71 of 700building tree 72 of 700\n",
      "\n",
      "building tree 73 of 700\n",
      "building tree 74 of 700\n",
      "building tree 75 of 700\n",
      "building tree 76 of 700\n",
      "building tree 77 of 700\n",
      "building tree 78 of 700\n",
      "building tree 79 of 700\n",
      "building tree 80 of 700\n",
      "building tree 81 of 700\n",
      "building tree 82 of 700\n",
      "building tree 83 of 700\n",
      "building tree 84 of 700\n",
      "building tree 85 of 700\n",
      "building tree 86 of 700\n",
      "building tree 87 of 700\n",
      "building tree 88 of 700\n",
      "building tree 89 of 700\n",
      "building tree 90 of 700\n",
      "building tree 91 of 700\n",
      "building tree 92 of 700\n",
      "building tree 93 of 700\n",
      "building tree 94 of 700\n",
      "building tree 95 of 700\n",
      "building tree 96 of 700\n",
      "building tree 97 of 700building tree 98 of 700\n",
      "\n",
      "building tree 99 of 700\n",
      "building tree 100 of 700\n",
      "building tree 101 of 700\n",
      "building tree 102 of 700\n",
      "building tree 103 of 700\n",
      "building tree 104 of 700\n",
      "building tree 105 of 700\n",
      "building tree 106 of 700\n",
      "building tree 107 of 700\n",
      "building tree 108 of 700\n",
      "building tree 109 of 700\n",
      "building tree 110 of 700\n",
      "building tree 111 of 700building tree 112 of 700\n",
      "\n",
      "building tree 113 of 700\n",
      "building tree 114 of 700\n",
      "building tree 115 of 700\n",
      "building tree 116 of 700\n",
      "building tree 117 of 700\n",
      "building tree 118 of 700\n",
      "building tree 119 of 700\n",
      "building tree 120 of 700\n",
      "building tree 121 of 700\n",
      "building tree 122 of 700\n",
      "building tree 123 of 700\n",
      "building tree 124 of 700\n",
      "building tree 125 of 700\n",
      "building tree 126 of 700\n",
      "building tree 127 of 700\n",
      "building tree 128 of 700\n",
      "building tree 129 of 700\n",
      "building tree 130 of 700\n",
      "building tree 131 of 700building tree 132 of 700\n",
      "\n",
      "building tree 133 of 700\n",
      "building tree 134 of 700\n",
      "building tree 135 of 700\n",
      "building tree 136 of 700\n",
      "building tree 137 of 700\n",
      "building tree 138 of 700\n",
      "building tree 139 of 700\n",
      "building tree 140 of 700\n",
      "building tree 141 of 700\n",
      "building tree 142 of 700\n",
      "building tree 143 of 700\n",
      "building tree 144 of 700\n",
      "building tree 145 of 700\n",
      "building tree 146 of 700\n",
      "building tree 147 of 700\n",
      "building tree 148 of 700\n",
      "building tree 149 of 700\n",
      "building tree 150 of 700\n",
      "building tree 151 of 700\n",
      "building tree 152 of 700\n",
      "building tree 153 of 700\n",
      "building tree 154 of 700\n",
      "building tree 155 of 700\n",
      "building tree 156 of 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:   31.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 157 of 700\n",
      "building tree 158 of 700\n",
      "building tree 159 of 700\n",
      "building tree 160 of 700\n",
      "building tree 161 of 700\n",
      "building tree 162 of 700\n",
      "building tree 163 of 700\n",
      "building tree 164 of 700\n",
      "building tree 165 of 700\n",
      "building tree 166 of 700\n",
      "building tree 167 of 700\n",
      "building tree 168 of 700\n",
      "building tree 169 of 700\n",
      "building tree 170 of 700\n",
      "building tree 171 of 700\n",
      "building tree 172 of 700\n",
      "building tree 173 of 700\n",
      "building tree 174 of 700\n",
      "building tree 175 of 700\n",
      "building tree 176 of 700\n",
      "building tree 177 of 700\n",
      "building tree 178 of 700\n",
      "building tree 179 of 700\n",
      "building tree 180 of 700\n",
      "building tree 181 of 700\n",
      "building tree 182 of 700\n",
      "building tree 183 of 700\n",
      "building tree 184 of 700\n",
      "building tree 185 of 700\n",
      "building tree 186 of 700\n",
      "building tree 187 of 700\n",
      "building tree 188 of 700\n",
      "building tree 189 of 700\n",
      "building tree 190 of 700\n",
      "building tree 191 of 700\n",
      "building tree 192 of 700\n",
      "building tree 193 of 700\n",
      "building tree 194 of 700\n",
      "building tree 195 of 700\n",
      "building tree 196 of 700\n",
      "building tree 197 of 700\n",
      "building tree 198 of 700\n",
      "building tree 199 of 700\n",
      "building tree 200 of 700\n",
      "building tree 201 of 700\n",
      "building tree 202 of 700\n",
      "building tree 203 of 700\n",
      "building tree 204 of 700\n",
      "building tree 205 of 700\n",
      "building tree 206 of 700\n",
      "building tree 207 of 700\n",
      "building tree 208 of 700\n",
      "building tree 209 of 700\n",
      "building tree 210 of 700\n",
      "building tree 211 of 700\n",
      "building tree 212 of 700\n",
      "building tree 213 of 700\n",
      "building tree 214 of 700\n",
      "building tree 215 of 700\n",
      "building tree 216 of 700\n",
      "building tree 217 of 700\n",
      "building tree 218 of 700\n",
      "building tree 219 of 700\n",
      "building tree 220 of 700\n",
      "building tree 221 of 700\n",
      "building tree 222 of 700\n",
      "building tree 223 of 700\n",
      "building tree 224 of 700\n",
      "building tree 225 of 700\n",
      "building tree 226 of 700\n",
      "building tree 227 of 700\n",
      "building tree 228 of 700\n",
      "building tree 229 of 700\n",
      "building tree 230 of 700\n",
      "building tree 231 of 700\n",
      "building tree 232 of 700\n",
      "building tree 233 of 700\n",
      "building tree 234 of 700\n",
      "building tree 235 of 700\n",
      "building tree 236 of 700\n",
      "building tree 237 of 700\n",
      "building tree 238 of 700\n",
      "building tree 239 of 700\n",
      "building tree 240 of 700\n",
      "building tree 241 of 700\n",
      "building tree 242 of 700\n",
      "building tree 243 of 700\n",
      "building tree 244 of 700\n",
      "building tree 245 of 700\n",
      "building tree 246 of 700\n",
      "building tree 247 of 700\n",
      "building tree 248 of 700\n",
      "building tree 249 of 700\n",
      "building tree 250 of 700\n",
      "building tree 251 of 700\n",
      "building tree 252 of 700\n",
      "building tree 253 of 700\n",
      "building tree 254 of 700\n",
      "building tree 255 of 700\n",
      "building tree 256 of 700\n",
      "building tree 257 of 700\n",
      "building tree 258 of 700\n",
      "building tree 259 of 700\n",
      "building tree 260 of 700\n",
      "building tree 261 of 700\n",
      "building tree 262 of 700\n",
      "building tree 263 of 700\n",
      "building tree 264 of 700\n",
      "building tree 265 of 700\n",
      "building tree 266 of 700\n",
      "building tree 267 of 700\n",
      "building tree 268 of 700\n",
      "building tree 269 of 700\n",
      "building tree 270 of 700\n",
      "building tree 271 of 700\n",
      "building tree 272 of 700\n",
      "building tree 273 of 700\n",
      "building tree 274 of 700\n",
      "building tree 275 of 700\n",
      "building tree 276 of 700\n",
      "building tree 277 of 700\n",
      "building tree 278 of 700\n",
      "building tree 279 of 700\n",
      "building tree 280 of 700\n",
      "building tree 281 of 700\n",
      "building tree 282 of 700\n",
      "building tree 283 of 700\n",
      "building tree 284 of 700\n",
      "building tree 285 of 700\n",
      "building tree 286 of 700\n",
      "building tree 287 of 700\n",
      "building tree 288 of 700\n",
      "building tree 289 of 700\n",
      "building tree 290 of 700\n",
      "building tree 291 of 700\n",
      "building tree 292 of 700\n",
      "building tree 293 of 700\n",
      "building tree 294 of 700\n",
      "building tree 295 of 700\n",
      "building tree 296 of 700\n",
      "building tree 297 of 700\n",
      "building tree 298 of 700\n",
      "building tree 299 of 700\n",
      "building tree 300 of 700\n",
      "building tree 301 of 700\n",
      "building tree 302 of 700\n",
      "building tree 303 of 700\n",
      "building tree 304 of 700\n",
      "building tree 305 of 700\n",
      "building tree 306 of 700\n",
      "building tree 307 of 700\n",
      "building tree 308 of 700\n",
      "building tree 309 of 700\n",
      "building tree 310 of 700\n",
      "building tree 311 of 700\n",
      "building tree 312 of 700\n",
      "building tree 313 of 700\n",
      "building tree 314 of 700\n",
      "building tree 315 of 700\n",
      "building tree 316 of 700\n",
      "building tree 317 of 700\n",
      "building tree 318 of 700\n",
      "building tree 319 of 700\n",
      "building tree 320 of 700\n",
      "building tree 321 of 700\n",
      "building tree 322 of 700\n",
      "building tree 323 of 700\n",
      "building tree 324 of 700\n",
      "building tree 325 of 700\n",
      "building tree 326 of 700\n",
      "building tree 327 of 700\n",
      "building tree 328 of 700\n",
      "building tree 329 of 700\n",
      "building tree 330 of 700\n",
      "building tree 331 of 700\n",
      "building tree 332 of 700\n",
      "building tree 333 of 700\n",
      "building tree 334 of 700\n",
      "building tree 335 of 700building tree 336 of 700\n",
      "\n",
      "building tree 337 of 700\n",
      "building tree 338 of 700\n",
      "building tree 339 of 700\n",
      "building tree 340 of 700\n",
      "building tree 341 of 700\n",
      "building tree 342 of 700\n",
      "building tree 343 of 700\n",
      "building tree 344 of 700\n",
      "building tree 345 of 700\n",
      "building tree 346 of 700\n",
      "building tree 347 of 700\n",
      "building tree 348 of 700\n",
      "building tree 349 of 700\n",
      "building tree 350 of 700\n",
      "building tree 351 of 700\n",
      "building tree 352 of 700\n",
      "building tree 353 of 700\n",
      "building tree 354 of 700\n",
      "building tree 355 of 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 345 tasks      | elapsed:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 356 of 700\n",
      "building tree 357 of 700\n",
      "building tree 358 of 700\n",
      "building tree 359 of 700\n",
      "building tree 360 of 700\n",
      "building tree 361 of 700\n",
      "building tree 362 of 700\n",
      "building tree 363 of 700\n",
      "building tree 364 of 700\n",
      "building tree 365 of 700\n",
      "building tree 366 of 700\n",
      "building tree 367 of 700\n",
      "building tree 368 of 700\n",
      "building tree 369 of 700\n",
      "building tree 370 of 700\n",
      "building tree 371 of 700\n",
      "building tree 372 of 700\n",
      "building tree 373 of 700\n",
      "building tree 374 of 700\n",
      "building tree 375 of 700\n",
      "building tree 376 of 700\n",
      "building tree 377 of 700\n",
      "building tree 378 of 700\n",
      "building tree 379 of 700\n",
      "building tree 380 of 700\n",
      "building tree 381 of 700\n",
      "building tree 382 of 700\n",
      "building tree 383 of 700\n",
      "building tree 384 of 700\n",
      "building tree 385 of 700\n",
      "building tree 386 of 700\n",
      "building tree 387 of 700\n",
      "building tree 388 of 700\n",
      "building tree 389 of 700\n",
      "building tree 390 of 700\n",
      "building tree 391 of 700\n",
      "building tree 392 of 700\n",
      "building tree 393 of 700\n",
      "building tree 394 of 700\n",
      "building tree 395 of 700\n",
      "building tree 396 of 700\n",
      "building tree 397 of 700\n",
      "building tree 398 of 700\n",
      "building tree 399 of 700\n",
      "building tree 400 of 700\n",
      "building tree 401 of 700\n",
      "building tree 402 of 700\n",
      "building tree 403 of 700\n",
      "building tree 404 of 700\n",
      "building tree 405 of 700\n",
      "building tree 406 of 700\n",
      "building tree 407 of 700\n",
      "building tree 408 of 700\n",
      "building tree 409 of 700\n",
      "building tree 410 of 700\n",
      "building tree 411 of 700building tree 412 of 700\n",
      "\n",
      "building tree 413 of 700\n",
      "building tree 414 of 700\n",
      "building tree 415 of 700\n",
      "building tree 416 of 700\n",
      "building tree 417 of 700\n",
      "building tree 418 of 700\n",
      "building tree 419 of 700\n",
      "building tree 420 of 700\n",
      "building tree 421 of 700\n",
      "building tree 422 of 700\n",
      "building tree 423 of 700\n",
      "building tree 424 of 700\n",
      "building tree 425 of 700\n",
      "building tree 426 of 700\n",
      "building tree 427 of 700\n",
      "building tree 428 of 700\n",
      "building tree 429 of 700\n",
      "building tree 430 of 700\n",
      "building tree 431 of 700\n",
      "building tree 432 of 700\n",
      "building tree 433 of 700\n",
      "building tree 434 of 700\n",
      "building tree 435 of 700\n",
      "building tree 436 of 700\n",
      "building tree 437 of 700\n",
      "building tree 438 of 700\n",
      "building tree 439 of 700\n",
      "building tree 440 of 700\n",
      "building tree 441 of 700\n",
      "building tree 442 of 700\n",
      "building tree 443 of 700\n",
      "building tree 444 of 700\n",
      "building tree 445 of 700\n",
      "building tree 446 of 700\n",
      "building tree 447 of 700\n",
      "building tree 448 of 700\n",
      "building tree 449 of 700\n",
      "building tree 450 of 700\n",
      "building tree 451 of 700\n",
      "building tree 452 of 700\n",
      "building tree 453 of 700\n",
      "building tree 454 of 700\n",
      "building tree 455 of 700\n",
      "building tree 456 of 700\n",
      "building tree 457 of 700\n",
      "building tree 458 of 700\n",
      "building tree 459 of 700\n",
      "building tree 460 of 700\n",
      "building tree 461 of 700\n",
      "building tree 462 of 700\n",
      "building tree 463 of 700\n",
      "building tree 464 of 700\n",
      "building tree 465 of 700\n",
      "building tree 466 of 700\n",
      "building tree 467 of 700\n",
      "building tree 468 of 700\n",
      "building tree 469 of 700\n",
      "building tree 470 of 700\n",
      "building tree 471 of 700\n",
      "building tree 472 of 700\n",
      "building tree 473 of 700\n",
      "building tree 474 of 700\n",
      "building tree 475 of 700\n",
      "building tree 476 of 700\n",
      "building tree 477 of 700\n",
      "building tree 478 of 700\n",
      "building tree 479 of 700\n",
      "building tree 480 of 700\n",
      "building tree 481 of 700\n",
      "building tree 482 of 700\n",
      "building tree 483 of 700\n",
      "building tree 484 of 700\n",
      "building tree 485 of 700\n",
      "building tree 486 of 700\n",
      "building tree 487 of 700\n",
      "building tree 488 of 700\n",
      "building tree 489 of 700\n",
      "building tree 490 of 700\n",
      "building tree 491 of 700\n",
      "building tree 492 of 700\n",
      "building tree 493 of 700\n",
      "building tree 494 of 700\n",
      "building tree 495 of 700\n",
      "building tree 496 of 700\n",
      "building tree 497 of 700\n",
      "building tree 498 of 700\n",
      "building tree 499 of 700\n",
      "building tree 500 of 700\n",
      "building tree 501 of 700\n",
      "building tree 502 of 700\n",
      "building tree 503 of 700\n",
      "building tree 504 of 700\n",
      "building tree 505 of 700\n",
      "building tree 506 of 700\n",
      "building tree 507 of 700\n",
      "building tree 508 of 700\n",
      "building tree 509 of 700\n",
      "building tree 510 of 700\n",
      "building tree 511 of 700\n",
      "building tree 512 of 700\n",
      "building tree 513 of 700\n",
      "building tree 514 of 700\n",
      "building tree 515 of 700\n",
      "building tree 516 of 700\n",
      "building tree 517 of 700\n",
      "building tree 518 of 700\n",
      "building tree 519 of 700\n",
      "building tree 520 of 700\n",
      "building tree 521 of 700\n",
      "building tree 522 of 700\n",
      "building tree 523 of 700\n",
      "building tree 524 of 700\n",
      "building tree 525 of 700\n",
      "building tree 526 of 700\n",
      "building tree 527 of 700\n",
      "building tree 528 of 700\n",
      "building tree 529 of 700\n",
      "building tree 530 of 700\n",
      "building tree 531 of 700\n",
      "building tree 532 of 700\n",
      "building tree 533 of 700\n",
      "building tree 534 of 700\n",
      "building tree 535 of 700\n",
      "building tree 536 of 700\n",
      "building tree 537 of 700\n",
      "building tree 538 of 700\n",
      "building tree 539 of 700\n",
      "building tree 540 of 700\n",
      "building tree 541 of 700\n",
      "building tree 542 of 700\n",
      "building tree 543 of 700\n",
      "building tree 544 of 700\n",
      "building tree 545 of 700\n",
      "building tree 546 of 700\n",
      "building tree 547 of 700\n",
      "building tree 548 of 700\n",
      "building tree 549 of 700\n",
      "building tree 550 of 700\n",
      "building tree 551 of 700\n",
      "building tree 552 of 700\n",
      "building tree 553 of 700\n",
      "building tree 554 of 700\n",
      "building tree 555 of 700\n",
      "building tree 556 of 700\n",
      "building tree 557 of 700\n",
      "building tree 558 of 700\n",
      "building tree 559 of 700\n",
      "building tree 560 of 700\n",
      "building tree 561 of 700\n",
      "building tree 562 of 700\n",
      "building tree 563 of 700\n",
      "building tree 564 of 700\n",
      "building tree 565 of 700\n",
      "building tree 566 of 700\n",
      "building tree 567 of 700\n",
      "building tree 568 of 700\n",
      "building tree 569 of 700\n",
      "building tree 570 of 700\n",
      "building tree 571 of 700\n",
      "building tree 572 of 700\n",
      "building tree 573 of 700\n",
      "building tree 574 of 700\n",
      "building tree 575 of 700\n",
      "building tree 576 of 700\n",
      "building tree 577 of 700\n",
      "building tree 578 of 700\n",
      "building tree 579 of 700\n",
      "building tree 580 of 700\n",
      "building tree 581 of 700\n",
      "building tree 582 of 700\n",
      "building tree 583 of 700\n",
      "building tree 584 of 700\n",
      "building tree 585 of 700\n",
      "building tree 586 of 700\n",
      "building tree 587 of 700\n",
      "building tree 588 of 700\n",
      "building tree 589 of 700\n",
      "building tree 590 of 700\n",
      "building tree 591 of 700\n",
      "building tree 592 of 700\n",
      "building tree 593 of 700\n",
      "building tree 594 of 700\n",
      "building tree 595 of 700\n",
      "building tree 596 of 700\n",
      "building tree 597 of 700\n",
      "building tree 598 of 700\n",
      "building tree 599 of 700\n",
      "building tree 600 of 700\n",
      "building tree 601 of 700\n",
      "building tree 602 of 700\n",
      "building tree 603 of 700\n",
      "building tree 604 of 700\n",
      "building tree 605 of 700\n",
      "building tree 606 of 700\n",
      "building tree 607 of 700\n",
      "building tree 608 of 700\n",
      "building tree 609 of 700\n",
      "building tree 610 of 700\n",
      "building tree 611 of 700\n",
      "building tree 612 of 700\n",
      "building tree 613 of 700\n",
      "building tree 614 of 700\n",
      "building tree 615 of 700\n",
      "building tree 616 of 700\n",
      "building tree 617 of 700\n",
      "building tree 618 of 700\n",
      "building tree 619 of 700\n",
      "building tree 620 of 700\n",
      "building tree 621 of 700\n",
      "building tree 622 of 700\n",
      "building tree 623 of 700\n",
      "building tree 624 of 700\n",
      "building tree 625 of 700\n",
      "building tree 626 of 700\n",
      "building tree 627 of 700\n",
      "building tree 628 of 700\n",
      "building tree 629 of 700\n",
      "building tree 630 of 700\n",
      "building tree 631 of 700\n",
      "building tree 632 of 700\n",
      "building tree 633 of 700\n",
      "building tree 634 of 700\n",
      "building tree 635 of 700\n",
      "building tree 636 of 700\n",
      "building tree 637 of 700\n",
      "building tree 638 of 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 628 tasks      | elapsed:  2.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 639 of 700\n",
      "building tree 640 of 700\n",
      "building tree 641 of 700\n",
      "building tree 642 of 700\n",
      "building tree 643 of 700\n",
      "building tree 644 of 700\n",
      "building tree 645 of 700\n",
      "building tree 646 of 700\n",
      "building tree 647 of 700\n",
      "building tree 648 of 700\n",
      "building tree 649 of 700\n",
      "building tree 650 of 700\n",
      "building tree 651 of 700\n",
      "building tree 652 of 700\n",
      "building tree 653 of 700\n",
      "building tree 654 of 700\n",
      "building tree 655 of 700\n",
      "building tree 656 of 700\n",
      "building tree 657 of 700\n",
      "building tree 658 of 700\n",
      "building tree 659 of 700\n",
      "building tree 660 of 700\n",
      "building tree 661 of 700\n",
      "building tree 662 of 700\n",
      "building tree 663 of 700\n",
      "building tree 664 of 700\n",
      "building tree 665 of 700\n",
      "building tree 666 of 700\n",
      "building tree 667 of 700\n",
      "building tree 668 of 700\n",
      "building tree 669 of 700\n",
      "building tree 670 of 700\n",
      "building tree 671 of 700\n",
      "building tree 672 of 700\n",
      "building tree 673 of 700\n",
      "building tree 674 of 700\n",
      "building tree 675 of 700\n",
      "building tree 676 of 700\n",
      "building tree 677 of 700\n",
      "building tree 678 of 700\n",
      "building tree 679 of 700\n",
      "building tree 680 of 700\n",
      "building tree 681 of 700\n",
      "building tree 682 of 700\n",
      "building tree 683 of 700\n",
      "building tree 684 of 700\n",
      "building tree 685 of 700\n",
      "building tree 686 of 700\n",
      "building tree 687 of 700\n",
      "building tree 688 of 700\n",
      "building tree 689 of 700\n",
      "building tree 690 of 700\n",
      "building tree 691 of 700\n",
      "building tree 692 of 700\n",
      "building tree 693 of 700\n",
      "building tree 694 of 700\n",
      "building tree 695 of 700\n",
      "building tree 696 of 700\n",
      "building tree 697 of 700\n",
      "building tree 698 of 700\n",
      "building tree 699 of 700\n",
      "building tree 700 of 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 142 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 345 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=10)]: Done 628 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=10)]: Done 700 out of 700 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "prediction = history.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.7594560113417498\n",
      "mse: 106657.97968453912\n"
     ]
    }
   ],
   "source": [
    "print(f\"r2: {r2_score(y_valid, prediction)}\")                                  \n",
    "print(f\"mse: {mean_squared_error(y_valid, prediction)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make prediction on final test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 700building tree 2 of 700\n",
      "building tree 3 of 700\n",
      "\n",
      "building tree 4 of 700\n",
      "building tree 5 of 700\n",
      "building tree 6 of 700\n",
      "building tree 7 of 700\n",
      "building tree 8 of 700\n",
      "building tree 9 of 700\n",
      "building tree 10 of 700\n",
      "building tree 11 of 700\n",
      "building tree 12 of 700\n",
      "building tree 13 of 700\n",
      "building tree 14 of 700\n",
      "building tree 15 of 700\n",
      "building tree 16 of 700\n",
      "building tree 17 of 700\n",
      "building tree 18 of 700\n",
      "building tree 19 of 700building tree 20 of 700\n",
      "\n",
      "building tree 21 of 700\n",
      "building tree 22 of 700\n",
      "building tree 23 of 700\n",
      "building tree 24 of 700\n",
      "building tree 25 of 700\n",
      "building tree 26 of 700\n",
      "building tree 27 of 700\n",
      "building tree 28 of 700\n",
      "building tree 29 of 700\n",
      "building tree 30 of 700\n",
      "building tree 31 of 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    5.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 32 of 700\n",
      "building tree 33 of 700\n",
      "building tree 34 of 700\n",
      "building tree 35 of 700\n",
      "building tree 36 of 700\n",
      "building tree 37 of 700\n",
      "building tree 38 of 700\n",
      "building tree 39 of 700\n",
      "building tree 40 of 700\n",
      "building tree 41 of 700\n",
      "building tree 42 of 700\n",
      "building tree 43 of 700\n",
      "building tree 44 of 700\n",
      "building tree 45 of 700\n",
      "building tree 46 of 700\n",
      "building tree 47 of 700\n",
      "building tree 48 of 700\n",
      "building tree 49 of 700\n",
      "building tree 50 of 700\n",
      "building tree 51 of 700\n",
      "building tree 52 of 700\n",
      "building tree 53 of 700\n",
      "building tree 54 of 700\n",
      "building tree 55 of 700\n",
      "building tree 56 of 700\n",
      "building tree 57 of 700\n",
      "building tree 58 of 700\n",
      "building tree 59 of 700\n",
      "building tree 60 of 700\n",
      "building tree 61 of 700\n",
      "building tree 62 of 700\n",
      "building tree 63 of 700\n",
      "building tree 64 of 700\n",
      "building tree 65 of 700\n",
      "building tree 66 of 700\n",
      "building tree 67 of 700\n",
      "building tree 68 of 700\n",
      "building tree 69 of 700\n",
      "building tree 70 of 700\n",
      "building tree 71 of 700\n",
      "building tree 72 of 700\n",
      "building tree 73 of 700\n",
      "building tree 74 of 700\n",
      "building tree 75 of 700\n",
      "building tree 76 of 700\n",
      "building tree 77 of 700\n",
      "building tree 78 of 700\n",
      "building tree 79 of 700\n",
      "building tree 80 of 700\n",
      "building tree 81 of 700\n",
      "building tree 82 of 700\n",
      "building tree 83 of 700\n",
      "building tree 84 of 700\n",
      "building tree 85 of 700\n",
      "building tree 86 of 700\n",
      "building tree 87 of 700\n",
      "building tree 88 of 700\n",
      "building tree 89 of 700\n",
      "building tree 90 of 700\n",
      "building tree 91 of 700\n",
      "building tree 92 of 700\n",
      "building tree 93 of 700\n",
      "building tree 94 of 700\n",
      "building tree 95 of 700\n",
      "building tree 96 of 700\n",
      "building tree 97 of 700\n",
      "building tree 98 of 700\n",
      "building tree 99 of 700\n",
      "building tree 100 of 700\n",
      "building tree 101 of 700\n",
      "building tree 102 of 700\n",
      "building tree 103 of 700\n",
      "building tree 104 of 700\n",
      "building tree 105 of 700\n",
      "building tree 106 of 700\n",
      "building tree 107 of 700\n",
      "building tree 108 of 700\n",
      "building tree 109 of 700\n",
      "building tree 110 of 700\n",
      "building tree 111 of 700\n",
      "building tree 112 of 700\n",
      "building tree 113 of 700\n",
      "building tree 114 of 700\n",
      "building tree 115 of 700\n",
      "building tree 116 of 700\n",
      "building tree 117 of 700\n",
      "building tree 118 of 700\n",
      "building tree 119 of 700\n",
      "building tree 120 of 700\n",
      "building tree 121 of 700\n",
      "building tree 122 of 700\n",
      "building tree 123 of 700\n",
      "building tree 124 of 700\n",
      "building tree 125 of 700\n",
      "building tree 126 of 700\n",
      "building tree 127 of 700\n",
      "building tree 128 of 700\n",
      "building tree 129 of 700\n",
      "building tree 130 of 700\n",
      "building tree 131 of 700\n",
      "building tree 132 of 700\n",
      "building tree 133 of 700\n",
      "building tree 134 of 700\n",
      "building tree 135 of 700\n",
      "building tree 136 of 700\n",
      "building tree 137 of 700\n",
      "building tree 138 of 700\n",
      "building tree 139 of 700\n",
      "building tree 140 of 700\n",
      "building tree 141 of 700\n",
      "building tree 142 of 700\n",
      "building tree 143 of 700\n",
      "building tree 144 of 700\n",
      "building tree 145 of 700\n",
      "building tree 146 of 700\n",
      "building tree 147 of 700\n",
      "building tree 148 of 700\n",
      "building tree 149 of 700\n",
      "building tree 150 of 700\n",
      "building tree 151 of 700\n",
      "building tree 152 of 700\n",
      "building tree 153 of 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:   28.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 154 of 700\n",
      "building tree 155 of 700\n",
      "building tree 156 of 700\n",
      "building tree 157 of 700\n",
      "building tree 158 of 700\n",
      "building tree 159 of 700\n",
      "building tree 160 of 700\n",
      "building tree 161 of 700\n",
      "building tree 162 of 700building tree 163 of 700\n",
      "\n",
      "building tree 164 of 700\n",
      "building tree 165 of 700\n",
      "building tree 166 of 700\n",
      "building tree 167 of 700\n",
      "building tree 168 of 700\n",
      "building tree 169 of 700\n",
      "building tree 170 of 700\n",
      "building tree 171 of 700\n",
      "building tree 172 of 700\n",
      "building tree 173 of 700\n",
      "building tree 174 of 700\n",
      "building tree 175 of 700\n",
      "building tree 176 of 700\n",
      "building tree 177 of 700\n",
      "building tree 178 of 700\n",
      "building tree 179 of 700\n",
      "building tree 180 of 700\n",
      "building tree 181 of 700\n",
      "building tree 182 of 700\n",
      "building tree 183 of 700\n",
      "building tree 184 of 700\n",
      "building tree 185 of 700\n",
      "building tree 186 of 700\n",
      "building tree 187 of 700\n",
      "building tree 188 of 700\n",
      "building tree 189 of 700\n",
      "building tree 190 of 700\n",
      "building tree 191 of 700\n",
      "building tree 192 of 700\n",
      "building tree 193 of 700\n",
      "building tree 194 of 700\n",
      "building tree 195 of 700\n",
      "building tree 196 of 700\n",
      "building tree 197 of 700\n",
      "building tree 198 of 700\n",
      "building tree 199 of 700\n",
      "building tree 200 of 700\n",
      "building tree 201 of 700\n",
      "building tree 202 of 700\n",
      "building tree 203 of 700\n",
      "building tree 204 of 700\n",
      "building tree 205 of 700\n",
      "building tree 206 of 700\n",
      "building tree 207 of 700\n",
      "building tree 208 of 700\n",
      "building tree 209 of 700\n",
      "building tree 210 of 700\n",
      "building tree 211 of 700\n",
      "building tree 212 of 700\n",
      "building tree 213 of 700\n",
      "building tree 214 of 700\n",
      "building tree 215 of 700\n",
      "building tree 216 of 700\n",
      "building tree 217 of 700\n",
      "building tree 218 of 700\n",
      "building tree 219 of 700\n",
      "building tree 220 of 700\n",
      "building tree 221 of 700\n",
      "building tree 222 of 700\n",
      "building tree 223 of 700\n",
      "building tree 224 of 700\n",
      "building tree 225 of 700\n",
      "building tree 226 of 700\n",
      "building tree 227 of 700\n",
      "building tree 228 of 700\n",
      "building tree 229 of 700\n",
      "building tree 230 of 700\n",
      "building tree 231 of 700\n",
      "building tree 232 of 700\n",
      "building tree 233 of 700\n",
      "building tree 234 of 700\n",
      "building tree 235 of 700\n",
      "building tree 236 of 700\n",
      "building tree 237 of 700\n",
      "building tree 238 of 700\n",
      "building tree 239 of 700\n",
      "building tree 240 of 700\n",
      "building tree 241 of 700\n",
      "building tree 242 of 700\n",
      "building tree 243 of 700\n",
      "building tree 244 of 700\n",
      "building tree 245 of 700\n",
      "building tree 246 of 700\n",
      "building tree 247 of 700\n",
      "building tree 248 of 700\n",
      "building tree 249 of 700\n",
      "building tree 250 of 700\n",
      "building tree 251 of 700\n",
      "building tree 252 of 700\n",
      "building tree 253 of 700\n",
      "building tree 254 of 700\n",
      "building tree 255 of 700\n",
      "building tree 256 of 700\n",
      "building tree 257 of 700\n",
      "building tree 258 of 700\n",
      "building tree 259 of 700\n",
      "building tree 260 of 700\n",
      "building tree 261 of 700\n",
      "building tree 262 of 700\n",
      "building tree 263 of 700\n",
      "building tree 264 of 700\n",
      "building tree 265 of 700\n",
      "building tree 266 of 700\n",
      "building tree 267 of 700\n",
      "building tree 268 of 700\n",
      "building tree 269 of 700\n",
      "building tree 270 of 700\n",
      "building tree 271 of 700\n",
      "building tree 272 of 700\n",
      "building tree 273 of 700\n",
      "building tree 274 of 700\n",
      "building tree 275 of 700\n",
      "building tree 276 of 700\n",
      "building tree 277 of 700\n",
      "building tree 278 of 700\n",
      "building tree 279 of 700\n",
      "building tree 280 of 700\n",
      "building tree 281 of 700\n",
      "building tree 282 of 700\n",
      "building tree 283 of 700\n",
      "building tree 284 of 700\n",
      "building tree 285 of 700\n",
      "building tree 286 of 700\n",
      "building tree 287 of 700\n",
      "building tree 288 of 700\n",
      "building tree 289 of 700\n",
      "building tree 290 of 700\n",
      "building tree 291 of 700\n",
      "building tree 292 of 700\n",
      "building tree 293 of 700\n",
      "building tree 294 of 700\n",
      "building tree 295 of 700\n",
      "building tree 297 of 700\n",
      "building tree 296 of 700\n",
      "building tree 298 of 700\n",
      "building tree 299 of 700\n",
      "building tree 300 of 700\n",
      "building tree 301 of 700building tree 302 of 700\n",
      "\n",
      "building tree 303 of 700\n",
      "building tree 304 of 700\n",
      "building tree 305 of 700\n",
      "building tree 306 of 700\n",
      "building tree 307 of 700\n",
      "building tree 308 of 700\n",
      "building tree 309 of 700\n",
      "building tree 310 of 700\n",
      "building tree 311 of 700\n",
      "building tree 312 of 700\n",
      "building tree 313 of 700\n",
      "building tree 314 of 700\n",
      "building tree 315 of 700\n",
      "building tree 316 of 700\n",
      "building tree 317 of 700\n",
      "building tree 318 of 700\n",
      "building tree 319 of 700\n",
      "building tree 320 of 700\n",
      "building tree 321 of 700\n",
      "building tree 322 of 700\n",
      "building tree 323 of 700\n",
      "building tree 324 of 700\n",
      "building tree 325 of 700\n",
      "building tree 326 of 700\n",
      "building tree 327 of 700\n",
      "building tree 328 of 700\n",
      "building tree 329 of 700\n",
      "building tree 330 of 700\n",
      "building tree 331 of 700\n",
      "building tree 332 of 700\n",
      "building tree 333 of 700\n",
      "building tree 334 of 700\n",
      "building tree 335 of 700\n",
      "building tree 336 of 700\n",
      "building tree 337 of 700\n",
      "building tree 338 of 700\n",
      "building tree 339 of 700\n",
      "building tree 340 of 700\n",
      "building tree 341 of 700\n",
      "building tree 342 of 700\n",
      "building tree 343 of 700\n",
      "building tree 344 of 700\n",
      "building tree 345 of 700\n",
      "building tree 346 of 700\n",
      "building tree 347 of 700\n",
      "building tree 348 of 700\n",
      "building tree 349 of 700\n",
      "building tree 350 of 700\n",
      "building tree 351 of 700building tree 352 of 700\n",
      "\n",
      "building tree 353 of 700\n",
      "building tree 354 of 700\n",
      "building tree 355 of 700\n",
      "building tree 356 of 700\n",
      "building tree 357 of 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 345 tasks      | elapsed:  1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 358 of 700\n",
      "building tree 359 of 700\n",
      "building tree 360 of 700\n",
      "building tree 361 of 700\n",
      "building tree 362 of 700\n",
      "building tree 363 of 700\n",
      "building tree 364 of 700\n",
      "building tree 365 of 700\n",
      "building tree 366 of 700\n",
      "building tree 367 of 700\n",
      "building tree 368 of 700\n",
      "building tree 369 of 700\n",
      "building tree 370 of 700\n",
      "building tree 371 of 700\n",
      "building tree 372 of 700\n",
      "building tree 373 of 700\n",
      "building tree 374 of 700\n",
      "building tree 375 of 700\n",
      "building tree 376 of 700\n",
      "building tree 377 of 700\n",
      "building tree 378 of 700\n",
      "building tree 379 of 700\n",
      "building tree 380 of 700\n",
      "building tree 381 of 700\n",
      "building tree 382 of 700\n",
      "building tree 383 of 700\n",
      "building tree 384 of 700\n",
      "building tree 385 of 700\n",
      "building tree 386 of 700\n",
      "building tree 387 of 700\n",
      "building tree 388 of 700\n",
      "building tree 389 of 700\n",
      "building tree 390 of 700\n",
      "building tree 391 of 700\n",
      "building tree 392 of 700\n",
      "building tree 393 of 700\n",
      "building tree 394 of 700\n",
      "building tree 395 of 700\n",
      "building tree 396 of 700\n",
      "building tree 397 of 700\n",
      "building tree 398 of 700\n",
      "building tree 399 of 700\n",
      "building tree 400 of 700\n",
      "building tree 401 of 700\n",
      "building tree 402 of 700\n",
      "building tree 403 of 700\n",
      "building tree 404 of 700\n",
      "building tree 405 of 700\n",
      "building tree 406 of 700\n",
      "building tree 407 of 700\n",
      "building tree 408 of 700\n",
      "building tree 409 of 700\n",
      "building tree 410 of 700\n",
      "building tree 411 of 700\n",
      "building tree 412 of 700\n",
      "building tree 413 of 700\n",
      "building tree 414 of 700\n",
      "building tree 415 of 700\n",
      "building tree 416 of 700\n",
      "building tree 417 of 700\n",
      "building tree 418 of 700\n",
      "building tree 419 of 700\n",
      "building tree 420 of 700\n",
      "building tree 421 of 700\n",
      "building tree 422 of 700\n",
      "building tree 423 of 700\n",
      "building tree 424 of 700\n",
      "building tree 425 of 700\n",
      "building tree 426 of 700\n",
      "building tree 427 of 700\n",
      "building tree 428 of 700\n",
      "building tree 429 of 700\n",
      "building tree 430 of 700\n",
      "building tree 431 of 700\n",
      "building tree 432 of 700\n",
      "building tree 433 of 700\n",
      "building tree 434 of 700\n",
      "building tree 435 of 700\n",
      "building tree 436 of 700\n",
      "building tree 437 of 700\n",
      "building tree 438 of 700\n",
      "building tree 439 of 700\n",
      "building tree 440 of 700\n",
      "building tree 441 of 700\n",
      "building tree 442 of 700\n",
      "building tree 443 of 700\n",
      "building tree 444 of 700\n",
      "building tree 445 of 700\n",
      "building tree 446 of 700\n",
      "building tree 447 of 700\n",
      "building tree 448 of 700\n",
      "building tree 449 of 700\n",
      "building tree 450 of 700\n",
      "building tree 451 of 700\n",
      "building tree 452 of 700\n",
      "building tree 453 of 700\n",
      "building tree 454 of 700\n",
      "building tree 455 of 700\n",
      "building tree 456 of 700\n",
      "building tree 457 of 700\n",
      "building tree 458 of 700\n",
      "building tree 459 of 700\n",
      "building tree 460 of 700\n",
      "building tree 461 of 700\n",
      "building tree 462 of 700\n",
      "building tree 463 of 700\n",
      "building tree 464 of 700\n",
      "building tree 465 of 700\n",
      "building tree 466 of 700\n",
      "building tree 467 of 700\n",
      "building tree 468 of 700\n",
      "building tree 469 of 700\n",
      "building tree 470 of 700\n",
      "building tree 471 of 700\n",
      "building tree 472 of 700\n",
      "building tree 473 of 700\n",
      "building tree 474 of 700\n",
      "building tree 475 of 700\n",
      "building tree 476 of 700\n",
      "building tree 477 of 700\n",
      "building tree 478 of 700\n",
      "building tree 479 of 700\n",
      "building tree 480 of 700\n",
      "building tree 481 of 700\n",
      "building tree 482 of 700\n",
      "building tree 483 of 700\n",
      "building tree 484 of 700\n",
      "building tree 485 of 700\n",
      "building tree 486 of 700\n",
      "building tree 487 of 700\n",
      "building tree 488 of 700\n",
      "building tree 489 of 700\n",
      "building tree 490 of 700\n",
      "building tree 491 of 700\n",
      "building tree 492 of 700\n",
      "building tree 493 of 700\n",
      "building tree 494 of 700\n",
      "building tree 495 of 700\n",
      "building tree 496 of 700\n",
      "building tree 497 of 700\n",
      "building tree 498 of 700\n",
      "building tree 499 of 700\n",
      "building tree 500 of 700\n",
      "building tree 501 of 700\n",
      "building tree 502 of 700\n",
      "building tree 503 of 700\n",
      "building tree 504 of 700\n",
      "building tree 505 of 700\n",
      "building tree 506 of 700\n",
      "building tree 507 of 700\n",
      "building tree 508 of 700\n",
      "building tree 509 of 700\n",
      "building tree 510 of 700\n",
      "building tree 511 of 700\n",
      "building tree 512 of 700\n",
      "building tree 513 of 700\n",
      "building tree 514 of 700\n",
      "building tree 515 of 700\n",
      "building tree 516 of 700\n",
      "building tree 517 of 700\n",
      "building tree 518 of 700\n",
      "building tree 519 of 700\n",
      "building tree 520 of 700\n",
      "building tree 521 of 700\n",
      "building tree 522 of 700\n",
      "building tree 523 of 700\n",
      "building tree 524 of 700\n",
      "building tree 525 of 700\n",
      "building tree 526 of 700\n",
      "building tree 527 of 700\n",
      "building tree 528 of 700\n",
      "building tree 529 of 700\n",
      "building tree 530 of 700\n",
      "building tree 531 of 700\n",
      "building tree 532 of 700\n",
      "building tree 533 of 700\n",
      "building tree 534 of 700\n",
      "building tree 535 of 700\n",
      "building tree 536 of 700\n",
      "building tree 537 of 700\n",
      "building tree 538 of 700\n",
      "building tree 539 of 700\n",
      "building tree 540 of 700\n",
      "building tree 541 of 700\n",
      "building tree 542 of 700\n",
      "building tree 543 of 700\n",
      "building tree 544 of 700\n",
      "building tree 545 of 700\n",
      "building tree 546 of 700\n",
      "building tree 547 of 700\n",
      "building tree 548 of 700\n",
      "building tree 549 of 700\n",
      "building tree 550 of 700\n",
      "building tree 551 of 700\n",
      "building tree 552 of 700\n",
      "building tree 553 of 700\n",
      "building tree 554 of 700\n",
      "building tree 555 of 700\n",
      "building tree 556 of 700\n",
      "building tree 557 of 700\n",
      "building tree 558 of 700\n",
      "building tree 559 of 700\n",
      "building tree 560 of 700\n",
      "building tree 561 of 700\n",
      "building tree 562 of 700\n",
      "building tree 563 of 700\n",
      "building tree 564 of 700\n",
      "building tree 565 of 700\n",
      "building tree 566 of 700\n",
      "building tree 567 of 700\n",
      "building tree 568 of 700\n",
      "building tree 569 of 700\n",
      "building tree 570 of 700\n",
      "building tree 571 of 700\n",
      "building tree 572 of 700\n",
      "building tree 573 of 700\n",
      "building tree 574 of 700\n",
      "building tree 575 of 700\n",
      "building tree 576 of 700\n",
      "building tree 577 of 700\n",
      "building tree 578 of 700\n",
      "building tree 579 of 700\n",
      "building tree 580 of 700\n",
      "building tree 581 of 700\n",
      "building tree 582 of 700\n",
      "building tree 583 of 700\n",
      "building tree 584 of 700\n",
      "building tree 585 of 700\n",
      "building tree 586 of 700\n",
      "building tree 587 of 700\n",
      "building tree 588 of 700\n",
      "building tree 589 of 700\n",
      "building tree 590 of 700\n",
      "building tree 591 of 700\n",
      "building tree 592 of 700\n",
      "building tree 593 of 700\n",
      "building tree 594 of 700\n",
      "building tree 595 of 700\n",
      "building tree 596 of 700\n",
      "building tree 597 of 700\n",
      "building tree 598 of 700\n",
      "building tree 599 of 700\n",
      "building tree 600 of 700\n",
      "building tree 601 of 700\n",
      "building tree 602 of 700\n",
      "building tree 603 of 700\n",
      "building tree 604 of 700\n",
      "building tree 605 of 700\n",
      "building tree 606 of 700\n",
      "building tree 607 of 700\n",
      "building tree 608 of 700\n",
      "building tree 609 of 700\n",
      "building tree 610 of 700\n",
      "building tree 611 of 700\n",
      "building tree 612 of 700\n",
      "building tree 613 of 700\n",
      "building tree 614 of 700\n",
      "building tree 615 of 700\n",
      "building tree 616 of 700\n",
      "building tree 617 of 700\n",
      "building tree 618 of 700\n",
      "building tree 619 of 700\n",
      "building tree 620 of 700\n",
      "building tree 621 of 700\n",
      "building tree 622 of 700\n",
      "building tree 623 of 700\n",
      "building tree 624 of 700\n",
      "building tree 625 of 700\n",
      "building tree 626 of 700building tree 627 of 700\n",
      "\n",
      "building tree 628 of 700\n",
      "building tree 629 of 700\n",
      "building tree 630 of 700\n",
      "building tree 631 of 700\n",
      "building tree 632 of 700\n",
      "building tree 633 of 700\n",
      "building tree 634 of 700\n",
      "building tree 635 of 700\n",
      "building tree 636 of 700\n",
      "building tree 637 of 700\n",
      "building tree 638 of 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 628 tasks      | elapsed:  2.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 639 of 700\n",
      "building tree 640 of 700\n",
      "building tree 641 of 700\n",
      "building tree 642 of 700\n",
      "building tree 643 of 700\n",
      "building tree 644 of 700\n",
      "building tree 645 of 700\n",
      "building tree 646 of 700\n",
      "building tree 647 of 700\n",
      "building tree 648 of 700\n",
      "building tree 649 of 700\n",
      "building tree 650 of 700\n",
      "building tree 651 of 700\n",
      "building tree 652 of 700\n",
      "building tree 653 of 700\n",
      "building tree 654 of 700\n",
      "building tree 655 of 700\n",
      "building tree 656 of 700\n",
      "building tree 657 of 700\n",
      "building tree 658 of 700\n",
      "building tree 659 of 700\n",
      "building tree 660 of 700\n",
      "building tree 661 of 700\n",
      "building tree 662 of 700\n",
      "building tree 663 of 700\n",
      "building tree 664 of 700\n",
      "building tree 665 of 700\n",
      "building tree 666 of 700\n",
      "building tree 667 of 700\n",
      "building tree 668 of 700\n",
      "building tree 669 of 700\n",
      "building tree 670 of 700\n",
      "building tree 671 of 700\n",
      "building tree 672 of 700\n",
      "building tree 673 of 700\n",
      "building tree 674 of 700\n",
      "building tree 675 of 700\n",
      "building tree 676 of 700\n",
      "building tree 677 of 700\n",
      "building tree 678 of 700\n",
      "building tree 679 of 700\n",
      "building tree 680 of 700\n",
      "building tree 681 of 700\n",
      "building tree 682 of 700\n",
      "building tree 683 of 700\n",
      "building tree 684 of 700\n",
      "building tree 685 of 700\n",
      "building tree 686 of 700\n",
      "building tree 687 of 700\n",
      "building tree 688 of 700\n",
      "building tree 689 of 700\n",
      "building tree 690 of 700\n",
      "building tree 691 of 700\n",
      "building tree 692 of 700\n",
      "building tree 693 of 700\n",
      "building tree 694 of 700\n",
      "building tree 695 of 700\n",
      "building tree 696 of 700\n",
      "building tree 697 of 700\n",
      "building tree 698 of 700\n",
      "building tree 699 of 700\n",
      "building tree 700 of 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  2.3min finished\n"
     ]
    }
   ],
   "source": [
    "# USE ALL TRAIN DATA X, NOT JUST X_TRAIN!\n",
    "history2 = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(data, model, missing):\n",
    "  yhat = model.predict(data)\n",
    "  temp1 = pd.DataFrame({'idx': data.index, 'values': yhat})\n",
    "  temp2 = pd.DataFrame({'idx': missing.index})\n",
    "  temp = pd.concat([temp1,temp2],axis=0).sort_values(by='idx')\n",
    "  if temp['idx'].duplicated().any() == True: print('ATTENTION: Some indexes are duplicated.') # flagging errors\n",
    "  yhat = temp['values'].to_list()\n",
    "  return(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beat/opt/miniconda3/envs/ENV_ML/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 142 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 345 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=10)]: Done 628 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=10)]: Done 700 out of 700 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame(Predict(df_test, history2, df_test_missing))\n",
    "submission.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(submission).to_csv(\"Prediction_RandomForest.csv\", index=True, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ENV_ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "625c17cfbfff9990fa542c2aff1e1752f7d197787a6e9f9aa7169e5c74c47f71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
